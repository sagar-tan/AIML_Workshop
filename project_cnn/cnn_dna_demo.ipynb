{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DNA Motif Detection — CNN Demo\n",
        "\n",
        "**Biological framing:** Promoters often contain short motifs (e.g. a TATA box). We want to predict **promoter vs non-promoter** from sequence alone.\n",
        "\n",
        "To keep this demo reproducible, we generate synthetic sequences:\n",
        "- **Positive**: contains motif `TATA`\n",
        "- **Negative**: random DNA\n",
        "\n",
        "### Pipeline\n",
        "1. DNA strings → one-hot tensor (length × 4)\n",
        "2. Model: small 1D CNN\n",
        "3. Evaluation: accuracy + loss curve\n",
        "4. Insight: convolution filters behave like motif detectors\n",
        "\n",
        "Runtime target: **< 30 seconds**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "rng = np.random.default_rng(RANDOM_STATE)\n",
        "torch.manual_seed(RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Generate synthetic DNA sequences\n",
        "\n",
        "ALPHABET = np.array(list(\"ATCG\"))\n",
        "MOTIF = \"TATA\"\n",
        "SEQ_LEN = 80\n",
        "\n",
        "\n",
        "def random_dna(n, length=SEQ_LEN):\n",
        "    return [\"\".join(rng.choice(ALPHABET, size=length)) for _ in range(n)]\n",
        "\n",
        "\n",
        "def insert_motif(seq, motif=MOTIF):\n",
        "    # Insert motif at a random position\n",
        "    pos = rng.integers(0, len(seq) - len(motif) + 1)\n",
        "    return seq[:pos] + motif + seq[pos + len(motif) :]\n",
        "\n",
        "\n",
        "def make_dataset(n_pos=600, n_neg=600):\n",
        "    neg = random_dna(n_neg)\n",
        "    pos = [insert_motif(s) for s in random_dna(n_pos)]\n",
        "    X = pos + neg\n",
        "    y = np.array([1] * n_pos + [0] * n_neg, dtype=np.int64)\n",
        "\n",
        "    # Shuffle\n",
        "    idx = rng.permutation(len(X))\n",
        "    X = [X[i] for i in idx]\n",
        "    y = y[idx]\n",
        "    return X, y\n",
        "\n",
        "\n",
        "seqs, labels = make_dataset()\n",
        "print(\"Example positive contains motif?\", MOTIF in seqs[np.argmax(labels)])\n",
        "print(\"Example negative contains motif?\", MOTIF in seqs[np.argmin(labels)])\n",
        "print(\"Total:\", len(seqs), \"Pos rate:\", labels.mean().round(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. DNA → one-hot tensor (length × 4)\n",
        "\n",
        "BASE_TO_IDX = {\"A\": 0, \"T\": 1, \"C\": 2, \"G\": 3}\n",
        "\n",
        "\n",
        "def one_hot_encode(seq):\n",
        "    x = np.zeros((len(seq), 4), dtype=np.float32)\n",
        "    for i, ch in enumerate(seq):\n",
        "        x[i, BASE_TO_IDX[ch]] = 1.0\n",
        "    return x\n",
        "\n",
        "\n",
        "class DNADataset(Dataset):\n",
        "    def __init__(self, seqs, labels):\n",
        "        self.X = np.stack([one_hot_encode(s) for s in seqs])  # (N, L, 4)\n",
        "        self.y = labels.astype(np.float32)  # BCE expects float targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # For Conv1d we want (channels, length)\n",
        "        x = torch.from_numpy(self.X[idx]).transpose(0, 1)  # (4, L)\n",
        "        y = torch.tensor(self.y[idx])\n",
        "        return x, y\n",
        "\n",
        "\n",
        "# quick sanity check\n",
        "d = DNADataset(seqs[:3], labels[:3])\n",
        "print(d[0][0].shape, d[0][1].item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Train/test split + loaders\n",
        "\n",
        "n = len(seqs)\n",
        "split = int(0.8 * n)\n",
        "train_seqs, test_seqs = seqs[:split], seqs[split:]\n",
        "train_y, test_y = labels[:split], labels[split:]\n",
        "\n",
        "train_ds = DNADataset(train_seqs, train_y)\n",
        "test_ds = DNADataset(test_seqs, test_y)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=128, shuffle=False)\n",
        "\n",
        "print(\"Train batches:\", len(train_loader), \"Test batches:\", len(test_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Small 1D CNN model\n",
        "\n",
        "class MotifCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=4, out_channels=16, kernel_size=8, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2),\n",
        "            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=8, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveMaxPool1d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(1)  # logits\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MotifCNN().to(device)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "print(\"Device:\", device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Training loop (few epochs)\n",
        "\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            logits = model(xb)\n",
        "            loss = loss_fn(logits, yb)\n",
        "            total_loss += loss.item() * xb.size(0)\n",
        "            preds = (torch.sigmoid(logits) > 0.5).long()\n",
        "            correct += (preds == yb.long()).sum().item()\n",
        "            total += xb.size(0)\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "test_accs = []\n",
        "\n",
        "EPOCHS = 5\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    running = 0.0\n",
        "    for xb, yb in train_loader:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss = loss_fn(logits, yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        running += loss.item() * xb.size(0)\n",
        "\n",
        "    train_loss = running / len(train_ds)\n",
        "    test_loss, test_acc = evaluate(test_loader)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accs.append(test_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch}/{EPOCHS} - train_loss={train_loss:.4f} test_loss={test_loss:.4f} test_acc={test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Plot loss curve\n",
        "\n",
        "plt.figure(figsize=(6, 3.5))\n",
        "plt.plot(train_losses, label=\"train\")\n",
        "plt.plot(test_losses, label=\"test\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"BCE loss\")\n",
        "plt.title(\"CNN training curve\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Final test accuracy:\", round(test_accs[-1], 3))"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
