{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Protein Family Classification — Transformer Encoder Demo\n",
        "\n",
        "**Biological framing:** Protein families can be distinguished by sequence patterns and long-range context.\n",
        "\n",
        "To keep this demo fast, we generate synthetic protein sequences:\n",
        "- **Class A**: contains a `CXXC` pattern (Cys-any-any-Cys)\n",
        "- **Class B**: contains a `GGG` pattern\n",
        "\n",
        "### Pipeline\n",
        "1. Protein strings → integer tokens → padded tensor\n",
        "2. Model: Embedding + positional encoding + TransformerEncoder\n",
        "3. Evaluation: accuracy + loss curve\n",
        "4. Insight: self-attention mixes information across the whole sequence\n",
        "\n",
        "Runtime target: **< 30 seconds**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "rng = np.random.default_rng(RANDOM_STATE)\n",
        "torch.manual_seed(RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Generate synthetic protein sequences\n",
        "\n",
        "AA = list(\"ACDEFGHIKLMNPQRSTVWY\")  # 20 standard amino acids\n",
        "\n",
        "\n",
        "def random_protein(n, length=120):\n",
        "    return [\"\".join(rng.choice(AA, size=length)) for _ in range(n)]\n",
        "\n",
        "\n",
        "def insert_pattern(seq, pattern_type):\n",
        "    if pattern_type == \"CXXC\":\n",
        "        # Insert C??C\n",
        "        i = int(rng.integers(0, len(seq) - 4))\n",
        "        mid = \"\".join(rng.choice(AA, size=2))\n",
        "        pat = \"C\" + mid + \"C\"\n",
        "        return seq[:i] + pat + seq[i + 4 :]\n",
        "    if pattern_type == \"GGG\":\n",
        "        i = int(rng.integers(0, len(seq) - 3))\n",
        "        return seq[:i] + \"GGG\" + seq[i + 3 :]\n",
        "    raise ValueError(\"unknown pattern\")\n",
        "\n",
        "\n",
        "def make_dataset(n_a=600, n_b=600, length=120):\n",
        "    a = [insert_pattern(s, \"CXXC\") for s in random_protein(n_a, length=length)]\n",
        "    b = [insert_pattern(s, \"GGG\") for s in random_protein(n_b, length=length)]\n",
        "    X = a + b\n",
        "    y = np.array([0] * n_a + [1] * n_b, dtype=np.int64)  # 0=A, 1=B\n",
        "\n",
        "    idx = rng.permutation(len(X))\n",
        "    X = [X[i] for i in idx]\n",
        "    y = y[idx]\n",
        "    return X, y\n",
        "\n",
        "\n",
        "seqs, labels = make_dataset()\n",
        "print(\"Example A has CXXC?\", \"C\" in seqs[np.argmin(labels)])\n",
        "print(\"Example B has GGG?\", \"GGG\" in seqs[np.argmax(labels)])\n",
        "print(\"Total:\", len(seqs), \"Class B rate:\", labels.mean().round(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Tokenization: amino acids → integers, pad to same length\n",
        "\n",
        "PAD = \"_\"\n",
        "vocab = [PAD] + AA\n",
        "stoi = {ch: i for i, ch in enumerate(vocab)}\n",
        "itos = {i: ch for ch, i in stoi.items()}\n",
        "\n",
        "MAX_LEN = 120  # fixed for this demo\n",
        "\n",
        "\n",
        "def tokenize(seq, max_len=MAX_LEN):\n",
        "    ids = [stoi[ch] for ch in seq[:max_len]]\n",
        "    if len(ids) < max_len:\n",
        "        ids = ids + [stoi[PAD]] * (max_len - len(ids))\n",
        "    return np.array(ids, dtype=np.int64)\n",
        "\n",
        "\n",
        "class ProteinDataset(Dataset):\n",
        "    def __init__(self, seqs, labels):\n",
        "        self.X = np.stack([tokenize(s) for s in seqs])  # (N, L)\n",
        "        self.y = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.from_numpy(self.X[idx]), torch.tensor(self.y[idx])\n",
        "\n",
        "\n",
        "d = ProteinDataset(seqs[:2], labels[:2])\n",
        "print(d[0][0].shape, d[0][1].item(), \"vocab size:\", len(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Train/test split + loaders\n",
        "\n",
        "n = len(seqs)\n",
        "split = int(0.8 * n)\n",
        "train_seqs, test_seqs = seqs[:split], seqs[split:]\n",
        "train_y, test_y = labels[:split], labels[split:]\n",
        "\n",
        "train_ds = ProteinDataset(train_seqs, train_y)\n",
        "test_ds = ProteinDataset(test_seqs, test_y)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=128, shuffle=False)\n",
        "\n",
        "print(\"Train batches:\", len(train_loader), \"Test batches:\", len(test_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Model: embedding + positional encoding + TransformerEncoder\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=MAX_LEN):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer(\"pe\", pe.unsqueeze(0))  # (1, L, D)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, L, D)\n",
        "        return x + self.pe[:, : x.size(1), :]\n",
        "\n",
        "\n",
        "class ProteinTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=64, nhead=4, num_layers=2, dim_ff=128, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=stoi[PAD])\n",
        "        self.pos = PositionalEncoding(d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_ff,\n",
        "            dropout=0.1,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.enc = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.head = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, ids):\n",
        "        # ids: (B, L)\n",
        "        x = self.embed(ids)  # (B, L, D)\n",
        "        x = self.pos(x)\n",
        "        x = self.enc(x)  # (B, L, D)\n",
        "        x = x.mean(dim=1)  # mean pooling\n",
        "        return self.head(x)  # (B, C)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ProteinTransformer(vocab_size=len(vocab)).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "opt = torch.optim.Adam(model.parameters(), lr=2e-3)\n",
        "\n",
        "print(\"Device:\", device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Training loop (few epochs)\n",
        "\n",
        "def eval_loader(loader):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            logits = model(xb)\n",
        "            loss = loss_fn(logits, yb)\n",
        "            total_loss += loss.item() * xb.size(0)\n",
        "            pred = logits.argmax(dim=1)\n",
        "            correct += (pred == yb).sum().item()\n",
        "            total += xb.size(0)\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "\n",
        "train_losses, test_losses, test_accs = [], [], []\n",
        "EPOCHS = 5\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    running = 0.0\n",
        "    for xb, yb in train_loader:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss = loss_fn(logits, yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        running += loss.item() * xb.size(0)\n",
        "\n",
        "    train_loss = running / len(train_ds)\n",
        "    test_loss, test_acc = eval_loader(test_loader)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accs.append(test_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch}/{EPOCHS} - train_loss={train_loss:.4f} test_loss={test_loss:.4f} test_acc={test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Plot loss curve\n",
        "\n",
        "plt.figure(figsize=(6, 3.5))\n",
        "plt.plot(train_losses, label=\"train\")\n",
        "plt.plot(test_losses, label=\"test\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Cross-entropy loss\")\n",
        "plt.title(\"Transformer training curve\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Final test accuracy:\", round(test_accs[-1], 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What to emphasize live\n",
        "\n",
        "- “Each position attends to every other position — that’s how Transformers capture long-range context.”\n",
        "- “We embed tokens, add positional information, and then apply self-attention layers.”\n",
        "- “Even in this toy dataset, the model learns that `CXXC` vs `GGG` patterns define families.”\n",
        "\n",
        "## How to extend\n",
        "- Increase encoder depth/width.\n",
        "- Replace synthetic sequences with a small FASTA in `data/`.\n",
        "- Fine-tune a pretrained protein language model.\n",
        "- Add multi-label classification and/or structural features."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
